import streamlit as st
import cv2
import tempfile
import numpy as np
import time
from ultralytics import YOLO
from utils import save_for_retrain, init_recorder
from streamlit_drawable_canvas import st_canvas
from PIL import Image

# ‚ö†Ô∏è ƒë·∫∑t ngay sau import
st.set_page_config(page_title="Wood Defect Detection", layout="wide")

st.title("üîç Wood Defect Detection System")

# Load model
if "model_path" not in st.session_state:
    st.session_state["model_path"] = "models/best.pt"
model = YOLO(st.session_state["model_path"])


with st.sidebar:
    uploaded_model = st.file_uploader(
        "üîÑ Ch·ªçn file model m·ªõi",
        type=["pt", "onnx", "engine", "tflite", "mlmodel", "pb", "torchscript", "ncnn"],
        key="model_file_uploader",
        help="Limit 200MB per file ‚Ä¢ PT, ONNX, ENGINE, TFLITE, MLMODEL, PB, TORCHSCRIPT, NCNN"
    )
    st.markdown(
        "<div style='color:gray;font-size:13px;margin-top:-10px;'>"
        "Drag and drop file here<br>Limit 200MB per file ‚Ä¢ PT, ONNX, ENGINE, "
        "TFLITE, MLMODEL, PB, TORCHSCRIPT, NCNN"
        "</div>",
        unsafe_allow_html=True
    )

    if uploaded_model is not None:
        import tempfile, shutil
        temp_model_path = tempfile.NamedTemporaryFile(
            delete=False,
            suffix="." + uploaded_model.name.split(".")[-1]
        ).name
        with open(temp_model_path, "wb") as f:
            shutil.copyfileobj(uploaded_model, f)
        st.session_state["model_path"] = temp_model_path
        st.success(f"‚úÖ ƒê√£ ch·ªçn model: {uploaded_model.name}")


# Sidebar
conf = st.sidebar.slider("Confidence", 0.1, 1.0, 0.25, 0.05)
source = st.sidebar.radio("Ch·ªçn ngu·ªìn d·ªØ li·ªáu", ["Camera", "Upload ·∫¢nh", "Upload Video"])
record = st.sidebar.checkbox("üé• Ghi l·∫°i qu√° tr√¨nh detect", value=False)

frame_placeholder = st.empty()
status_placeholder = st.empty()

# --- H√†m x·ª≠ l√Ω video/camera realtime ---
def process_video(cap, record=False):
    out, path = (None, None)
    if record:
        out, path = init_recorder()
        st.sidebar.success(f"ƒêang ghi: {path}")

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Detect
        results = model.predict(frame, conf=conf, imgsz=640, verbose=False)
        annotated = results[0].plot()

        # Hi·ªÉn th·ªã realtime
        frame_placeholder.image(annotated, channels="BGR", width=640)
        status_placeholder.write(f"‚è±Ô∏è {time.strftime('%H:%M:%S')}")

        # N·∫øu c√≥ ghi h√¨nh
        if record and out is not None:
            out.write(annotated)

        time.sleep(0.03)  # kho·∫£ng 30 FPS

    cap.release()
    if out: out.release()


# --- Camera Realtime ---
if source == "Camera":
    cap = cv2.VideoCapture(0)
    process_video(cap, record=record)

# --- Upload ·∫¢nh ---
# --- Upload ·∫¢nh ---
elif source == "Upload ·∫¢nh":
    uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh", type=["jpg", "jpeg", "png"])
    if uploaded_file:
        file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
        img = cv2.imdecode(file_bytes, 1)
        results = model.predict(img, conf=conf, imgsz=640, verbose=False)
        annotated = results[0].plot()

        st.image(annotated, channels="BGR", width=640, caption="K·∫øt qu·∫£ detect")

        # Feedback
        st.subheader("üìã ƒê√°nh gi√° k·∫øt qu·∫£")
        correct = st.radio("K·∫øt qu·∫£ detect c√≥ ƒë√∫ng kh√¥ng?", ["ƒê√∫ng", "Sai"], horizontal=True)

        if correct == "Sai":
            st.warning("‚ö†Ô∏è Vui l√≤ng ch·ªçn nh√£n v√† v·∫Ω nhi·ªÅu bbox (m·ªói bbox m·ªôt nh√£n, m√†u kh√°c nhau)")
            classes = model.names
            class_names = list(classes.values())
            # G√°n m√†u cho t·ª´ng nh√£n
            color_palette = ["#FF0000", "#00FF00", "#0000FF", "#FFA500", "#800080", "#00FFFF", "#FFC0CB", "#FFFF00"]
            color_map = {name: color_palette[i % len(color_palette)] for i, name in enumerate(class_names)}
            # Ch·ªçn nh√£n hi·ªán t·∫°i
            true_label = st.selectbox("Ch·ªçn nh√£n hi·ªán t·∫°i ƒë·ªÉ v·∫Ω bbox", options=class_names)
            # V·∫Ω bbox m·ªõi tr√™n ·∫£nh g·ªëc
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img_pil = Image.fromarray(img_rgb)
            canvas_result = st_canvas(
                fill_color=color_map[true_label] + "80",  # alpha
                stroke_width=3,
                stroke_color=color_map[true_label],
                background_image=img_pil,
                update_streamlit=True,
                height=img.shape[0],
                width=img.shape[1],
                drawing_mode="rect",
                key="canvas_multi_bbox"
            )
            if st.button("üíæ L∆∞u c√°c bbox"):
                if canvas_result.json_data and len(canvas_result.json_data["objects"]) > 0:
                    labels = []
                    objects = canvas_result.json_data["objects"]
                    # √Ånh x·∫° m√†u v·ªÅ nh√£n
                    color_to_class = {v.lower(): k for k, v in color_map.items()}
                    for obj in objects:
                        left = obj["left"]
                        top = obj["top"]
                        width_box = obj["width"]
                        height_box = obj["height"]
                        # Convert sang YOLO
                        x_center = (left + width_box / 2) / img.shape[1]
                        y_center = (top + height_box / 2) / img.shape[0]
                        w = width_box / img.shape[1]
                        h = height_box / img.shape[0]
                        # L·∫•y nh√£n t·ª´ m√†u v·∫Ω
                        stroke = obj.get("stroke", "#FF0000").lower()
                        # Lo·∫°i b·ªè alpha n·∫øu c√≥
                        if len(stroke) > 7:
                            stroke = stroke[:7]
                        cls_name = color_to_class.get(stroke, class_names[0])
                        cls_id = class_names.index(cls_name)
                        labels.append((cls_id, x_center, y_center, w, h))
                    img_path, label_path = save_for_retrain(img, labels=labels, class_names=class_names)
                    st.success(f"‚úÖ ƒê√£ l∆∞u {len(labels)} bbox v√†o {label_path}")
                else:
                    st.error("‚ùå B·∫°n c·∫ßn v·∫Ω √≠t nh·∫•t m·ªôt bbox tr∆∞·ªõc khi l∆∞u!")


# --- Upload Video ---
# --- Upload Video ---
elif source == "Upload Video":
    uploaded_video = st.file_uploader("Ch·ªçn video", type=["mp4", "avi", "mov", "mkv"])
    if uploaded_video:
        tfile = tempfile.NamedTemporaryFile(delete=False)
        tfile.write(uploaded_video.read())
        cap = cv2.VideoCapture(tfile.name)

        current_frame = None
        stop_frame = None

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            results = model.predict(frame, conf=conf, imgsz=640, verbose=False)
            annotated = results[0].plot()

            frame_placeholder.image(annotated, channels="BGR", width=640)
            status_placeholder.write(f"üé• ƒêang x·ª≠ l√Ω video... {time.strftime('%H:%M:%S')}")
            current_frame = frame.copy()

            # N√∫t ch·ª•p ·∫£nh t·∫°i frame hi·ªán t·∫°i
            if st.button("üì∏ Ch·ª•p frame n√†y"):
                stop_frame = current_frame
                break

        cap.release()

        # N·∫øu user ch·ª•p l·∫°i frame
        if stop_frame is not None:
            st.subheader("üñº Frame ƒë∆∞·ª£c ch·ª•p t·ª´ video")
            results = model.predict(stop_frame, conf=conf, imgsz=640, verbose=False)
            annotated = results[0].plot()

            st.image(annotated, channels="BGR", width=640, caption="K·∫øt qu·∫£ detect")

            correct = st.radio("K·∫øt qu·∫£ detect c√≥ ƒë√∫ng kh√¥ng?", ["ƒê√∫ng", "Sai"], horizontal=True)
            if correct == "Sai":
                st.warning("‚ö†Ô∏è Ch·ªçn nh√£n ƒë√∫ng v√† v·∫Ω l·∫°i bbox tr√™n frame")

                classes = model.names
                true_label = st.selectbox("Ch·ªçn nh√£n ƒë√∫ng", options=list(classes.values()))

                from streamlit_drawable_canvas import st_canvas
                from PIL import Image
                img_rgb = cv2.cvtColor(stop_frame, cv2.COLOR_BGR2RGB)
                img_pil = Image.fromarray(img_rgb)

                canvas_result = st_canvas(
                    fill_color="rgba(255, 0, 0, 0.3)",
                    stroke_width=3,
                    stroke_color="#FF0000",
                    background_image=img_pil,
                    update_streamlit=True,
                    height=stop_frame.shape[0],
                    width=stop_frame.shape[1],
                    drawing_mode="rect",
                    key="canvas_video_bbox"
                )

                if st.button("üíæ L∆∞u frame sai nh√£n"):
                    bbox = None
                    if canvas_result.json_data and len(canvas_result.json_data["objects"]) > 0:
                        obj = canvas_result.json_data["objects"][0]
                        left = obj["left"]
                        top = obj["top"]
                        width_box = obj["width"]
                        height_box = obj["height"]

                        # Chuy·ªÉn sang YOLO format
                        x_center = (left + width_box / 2) / stop_frame.shape[1]
                        y_center = (top + height_box / 2) / stop_frame.shape[0]
                        w = width_box / stop_frame.shape[1]
                        h = height_box / stop_frame.shape[0]
                        bbox = (x_center, y_center, w, h)

                    if bbox:
                        img_path, label_path = save_for_retrain(stop_frame, true_label, model.names, bbox)
                        st.success(f"‚úÖ ƒê√£ l∆∞u v√†o {img_path}, {label_path}")
                    else:
                        st.error("‚ùå B·∫°n c·∫ßn v·∫Ω m·ªôt bbox tr∆∞·ªõc khi l∆∞u!")
